output: Di seguito è riportata una breve descrizione degli elementi del log:

TensorFlow sta aprendo con successo le librerie dinamiche necessarie per funzionare con DirectML e DirectX 12 (dxgi.dll, d3d12.dll, directml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.dll), che sono utilizzate per sfruttare le risorse della GPU per l'elaborazione.
TensorFlow rileva che il tuo sistema ha un'unità di elaborazione AMD Radeon RX 5700 e la seleziona come dispositivo DirectML.
TensorFlow ti informa che il binario è ottimizzato per utilizzare le istruzioni AVX e AVX2 per le operazioni critiche per le prestazioni. Se vuoi utilizzare queste istruzioni anche in altre operazioni, dovrai ricompilare TensorFlow con le flag del compilatore appropriate.
Non riesce a identificare il nodo NUMA della GPU, quindi utilizza il valore predefinito 0. Questo non dovrebbe causare problemi significativi.
In sintesi, questo output indica che TensorFlow si sta inizializzando correttamente e utilizza la tua GPU AMD Radeon RX 5700 per le operazioni. Non ci sono errori evidenti nel log.

NUMA (Non-Uniform Memory Access) è un'architettura di memoria utilizzata nei sistemi multiprocessore per consentire a ciascun processore di accedere direttamente alla propria memoria locale e alla memoria di altri processori in modo più efficiente. In un sistema NUMA, i nodi rappresentano unità di elaborazione (processori o gruppi di processori) e le loro memorie locali associate.
Nel contesto delle GPU, un nodo NUMA si riferisce alla memoria associata a una specifica GPU e al processore a cui è connessa. Identificare correttamente il nodo NUMA della GPU può essere utile per ottimizzare l'allocazione della memoria e la comunicazione tra la GPU e la CPU, migliorando le prestazioni delle applicazioni che utilizzano entrambe le risorse.
Nel log che hai condiviso, TensorFlow non è stato in grado di identificare il nodo NUMA associato alla tua GPU e ha utilizzato il valore predefinito 0. Sebbene questo possa non essere ottimale per le prestazioni, non dovrebbe causare malfunzionamenti o errori nel funzionamento del tuo programma. In alcuni casi, la mancata identificazione del nodo NUMA può essere dovuta al fatto che il kernel del sistema operativo non è stato compilato con il supporto NUMA abilitato.

-----------------------------------------------------------------------

Creazione di diversi tipi di tensori:
Il codice inizia importando il modulo tensorflow e creando diversi tipi di tensori:
- `scalar` è uno scalare, ovvero un singolo numero;
- `vector` è un vettore, ovvero un insieme di numeri che rappresenta una grandezza con direzione, come la velocità e la direzione del vento;
- `matrix` è una matrice, ovvero un array bidimensionale di numeri;
- `another_matrix` è un'altra matrice, ma in questo caso, i numeri sono in virgola mobile e il tipo di dato è specificato come `tf.float16`;
- `tensor` è un tensore tridimensionale, ovvero un array di numeri organizzato in tre dimensioni.

-------------------------Creating tensors-----------------------------

Tensori costanti e variabili:
- `changable_tensor` è un tensore variabile, ovvero un tensore i cui valori possono essere modificati;
- `unchangable_tensor` è un tensore costante, ovvero un tensore i cui valori non possono essere modificati.
Successivamente, il codice mostra come modificare un valore all'interno del tensore variabile utilizzando il metodo `assign()`. 
Infine, viene mostrato che il tentativo di modificare un tensore costante utilizzando lo stesso metodo non funziona, poiché i tensori costanti non possono essere modificati.
Raramente mi capiterà di dover scegliere tra i due, in ogni caso è meglio usare constant.

-------------------------Random tensors---------------------------------

è utile per effettuare una prima randomica calibrazione dei tensori.

Una distribuzione uniforme è un tipo di distribuzione di probabilità in cui tutti gli esiti possibili hanno la stessa probabilità di verificarsi. In altre parole, tutti gli eventi sono equiprobabili. La distribuzione uniforme può essere continua o discreta, a seconda del tipo di variabile a cui si applica.
1. Distribuzione uniforme continua: Quando la variabile di interesse è continua, la distribuzione uniforme si riferisce a un intervallo finito di valori reali, in cui la probabilità di ogni valore all'interno dell'intervallo è la stessa. Ad esempio, la distribuzione uniforme continua tra 0 e 1 indica che la probabilità di selezionare un numero casuale in questo intervallo è costante. La funzione densità di probabilità (PDF) di una distribuzione uniforme continua è una funzione costante su un intervallo specificato e zero al di fuori di esso.
2. Distribuzione uniforme discreta: Quando la variabile di interesse è discreta, la distribuzione uniforme si riferisce a un insieme finito di valori, in cui la probabilità di ogni valore è la stessa. Un esempio comune di distribuzione uniforme discreta è il lancio di un dado equilibrato a sei facce: ogni faccia ha la stessa probabilità di 1/6 di apparire.
La distribuzione uniforme è spesso utilizzata come modello semplice e di base nelle simulazioni e nei test di vari processi, poiché ogni evento ha la stessa probabilità di verificarsi e nessun evento è favorito rispetto agli altri.

Il codice crea due tensori con valori casuali utilizzando TensorFlow e il modulo `tf.random.Generator`. In entrambi i casi, viene utilizzato lo stesso seed (42), che garantisce che la sequenza di numeri casuali generata sia la stessa se vengono utilizzati gli stessi metodi di generazione.
1. Nel primo blocco di codice, viene creato un tensore chiamato `random_1`:
   - `tf.random.Generator.from_seed(42)` inizializza un generatore di numeri casuali con il seed 42;
   - `random_1.normal(shape=(3,2))` genera un tensore di forma (3,2) con valori casuali estratti da una distribuzione normale (anche detta gaussiana). Infine, il tensore `random_1` viene stampato.
2. Nel secondo blocco di codice, viene creato un tensore chiamato `random_2`:
   - `tf.random.Generator.from_seed(42)` inizializza un altro generatore di numeri casuali con lo stesso seed 42;
   - `random_2.uniform(shape=(3,2))` genera un tensore di forma (3,2) con valori casuali estratti da una distribuzione uniforme. Infine, il tensore `random_2` viene stampato.
Sebbene entrambi i tensori utilizzino lo stesso seed, la generazione dei numeri casuali è diversa poiché `random_1` utilizza una distribuzione normale, mentre `random_2` utilizza una distribuzione uniforme. Pertanto, i valori all'interno di questi due tensori saranno diversi.

----------------------Shuffle the order of elements in a tensor-------------

Utile per far in modo che l'AI sia addestrata in maniera equa.
Se ad esempio dovessi allenarla per riconoscere la differenza tra ramen e spaghetti e 
le prima 10.000 immagini fossero tutte di ramen avrei un problema perchè l'AI all'inizio 
calibrerebbe il tensore per ricnoscere solo il ramen.
Eseguire uno shuffle quindi ci permetterebbe di allenare l'AI su entrambi i tipi di Ramen in
contemporanea.

Ovviamente se gli do un seed fisso la randomizzazione sarà sempre uguale come nel caso di shuffle_2

L'utilizzo di un seed globale è utile quando si vuole ottenere una riproducibilità dei risultati in diverse parti del codice, ad esempio quando si esegue un'operazione di addestramento di una rete neurale in più riprese. D'altra parte, l'utilizzo di un seed locale può essere utile quando si vuole controllare il comportamento di una singola operazione senza influenzare il resto del codice.
Se non è impostato né il seed globale né il seed dell'operazione: per questa operazione viene utilizzato un seed scelto casualmente.
Se il seed globale è impostato, ma il seed dell'operazione non lo è: il sistema seleziona deterministicamente un seed dell'operazione insieme al seed globale in modo da ottenere una sequenza casuale univoca. All'interno della stessa versione di tensorflow e del codice utente, questa sequenza è deterministica. Tuttavia, nelle diverse versioni, questa sequenza potrebbe cambiare. Se il codice dipende da particolari seed per funzionare, specificare in modo esplicito sia i seed globali che quelli a livello di operazione.
Se il seed dell'operazione è impostato, ma il seed globale non lo è: per determinare la sequenza casuale vengono utilizzati un seed globale predefinito e il seed dell'operazione specificato.
Se sono impostati sia il seed globale che quello dell'operazione: entrambi i semi vengono utilizzati insieme per determinare la sequenza casuale.

-----------------------Creating Array with numpy arrays------------------------

La differenza principale tra i tensor di Tensorflow e gli array di Numpy è che
i tensori hanno una meaggior velocità se esguiti tramite la GPU.

Tutto ciò che è scritto in numpy può essere riscritto con Tf.
Generalmente con numpy si mettonbo le variabili in minuscolo mentre con tensorflow in maiuscolo

Il codice crea tre tensori utilizzando TensorFlow e un array NumPy.

1. `x`: un tensore di dimensione 10x7, contenente solo valori 1.
2. `y`: un tensore di dimensione 4x3, contenente solo valori 0.
3. `numpy_A`: un array NumPy contenente numeri interi che vanno da 1 a 24 (estremi inclusi).
4. `A`: un tensore costante creato a partire dall'array `numpy_A`.
5. `B`: un tensore costante creato a partire dall'array `numpy_A` con una forma specifica (2x3x4).
6. `C`: un tensore costante con gli stessi valori del B m,a a cui è stata rimodellata la forma. (8x3 fa sempre 24, avrei potuto dargli anche  2x2x2x3 ad esempio)

OUTPUT A
tf.Tensor([ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24], shape=(24,), dtype=int32)

OUTPUT B
tf.Tensor(
[[[ 1  2  3  4]
  [ 5  6  7  8]
  [ 9 10 11 12]]

 [[13 14 15 16]
  [17 18 19 20]
  [21 22 23 24]]], shape=(2, 3, 4), dtype=int32)
OUTPUT C
tf.Tensor(
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]
 [13 14 15]
 [16 17 18]
 [19 20 21]
 [22 23 24]], shape=(8, 3), dtype=int32)

 --------------------Tensor attributes-------------------------------

Il codice crea un tensore di rango 4 e ne mostra alcune proprietà. Spiegherò ogni riga del codice:

1. `rank_4_tensor = tf.zeros(shape=[2,3,4,5])`: Crea un tensore di rango 4 con tutti gli elementi impostati a 0. La forma del tensore è [2, 3, 4, 5], che rappresenta 2 matrici, ognuna contenente 3 matrici di dimensione 4x5.
2. `print(rank_4_tensor)`: Stampa il tensore completo, mostrando la sua forma e i valori contenuti in esso.
3. `print(rank_4_tensor[0])`: Stampa solo la prima delle due matrici esterne (indice 0).
4. `print(rank_4_tensor[1][1][3][2])`: Accede a un singolo elemento del tensore usando gli indici [1, 1, 3, 2] e lo stampa. Poiché il tensore è inizializzato con tutti gli elementi impostati a 0, il risultato sarà 0.
5. `print(rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor))`: Stampa la forma, il rango e la dimensione del tensore. La forma è [2, 3, 4, 5], il rango è 4 (poiché ha 4 dimensioni), e la dimensione è il prodotto delle lunghezze delle dimensioni (ovvero 2 * 3 * 4 * 5 = 120), che rappresenta il numero totale di elementi nel tensore.
6. `print(rank_4_tensor.shape[-1])` rappresenta la forma dell'elemento -1
Il numero di dimensio in parole provere indica il numero di numeri da cui è composta la shape. es: shape=(7,8,9,6) abrà ndim = 4.

--------------------indexing and expanding tensors-------------------

Il codice crea un tensore di rango 4, esegue alcune operazioni di slicing e manipolazione delle dimensioni, e ne mostra i risultati. Ecco un riassunto delle operazioni:

1. Crea un tensore di rango 4 `rank_4_tensor` di forma [2, 3, 4, 5], con tutti gli elementi impostati a 0.
2. Crea un nuovo tensore `x` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `x` ha forma [2, 2, 2, 2].
3. Crea un nuovo tensore `y` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `y` ha forma [1, 1, 4, 5].
4. Crea un nuovo tensore `z` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `z` ha forma [1, 1, 4, 1].
5. Crea un tensore di rango 2 `rank_2_tensor` di forma [2, 2].
6. Crea un nuovo tensore `za` estraendo l'ultima colonna di `rank_2_tensor` utilizzando slicing; `za` ha forma [2].
7. Espande le dimensioni di `rank_2_tensor` utilizzando `tf.newaxis`, creando `rank_2_tensor_expanded` di forma [2, 2, 1].
8. Espande le dimensioni di `rank_2_tensor` utilizzando `tf.expand_dims`, creando `other_rank_2_tensor_expanded` di forma [2, 2, 1].

NB: ESAPANDERE LE DIMENSIONI NON VUOL DIRE AGGIUNGERE MATRICI, MA VUOL DIRE AGGIUNGERE 
DIMENSIONI AGLI ALEMENTI ESISTENTI, QUIDI ANDARLI A SCOMPATTARE IN SOTTOMATRICI.

------------------manipulating tensors with basic algebra--------------

1. La riga `tensor = tf.constant([[10,7],[3,4]])` crea un tensore costante con valori [[10,7],[3,4]] e lo assegna alla variabile `tensor`.
2. La riga `print(tensor + 10)` esegue un'operazione di addizione tra il tensore `tensor` e il valore scalare 10. Il risultato è un nuovo tensore in cui a ciascun elemento del tensore originale viene sommato 10. Viene stampato il risultato.
3. La riga `print(tensor * 100)` esegue un'operazione di moltiplicazione tra il tensore `tensor` e il valore scalare 100. Il risultato è un nuovo tensore in cui ciascun elemento del tensore originale viene moltiplicato per 100. Viene stampato il risultato.
4. La riga `print(tensor - 10)` esegue un'operazione di sottrazione tra il tensore `tensor` e il valore scalare 10. Il risultato è un nuovo tensore in cui a ciascun elemento del tensore originale viene sottratto 10. Viene stampato il risultato.
5. La riga `print(tf.multiply(tensor,10))` esegue un'operazione di moltiplicazione tra il tensore `tensor` e il valore scalare 10 utilizzando la funzione `tf.multiply()`. Il risultato è un nuovo tensore in cui ciascun elemento del tensore originale viene moltiplicato per 10. Viene stampato il risultato.

------------------matrix manipulation with tensor -------------------------

1. La riga `tensor = tf.constant([[10,7],[3,4]])` crea un tensore costante di dimensione 2x2 con i valori specificati e lo assegna alla variabile `tensor`.
2. La riga `x = tf.matmul(tensor, tensor)` esegue una moltiplicazione di matrici tra il tensore `tensor` e se stesso utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `x`.
3. La riga `print(x)` stampa il valore del tensore `x`, che è il risultato della moltiplicazione di matrici.
4. Le righe seguenti definiscono due nuovi tensori costanti: `tensor_1` e `tensor_2`.
5. La riga `y = tf.matmul(tensor_1, tensor_2)` esegue una moltiplicazione di matrici tra `tensor_1` e `tensor_2` utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `y`.
6. La riga `print(y)` stampa il valore del tensore `y`, che è il risultato della moltiplicazione di matrici.

È importante notare che la libreria TensorFlow supporta la notazione `@` per la moltiplicazione di matrici, quindi l'istruzione `y = tf.matmul(tensor_1, tensor_2)` può essere scritta anche come `y = tensor_1 @ tensor_2`.
Infine, come ho menzionato, per la moltiplicazione di matrici e la notazione puntata (dot notation), valgono le stesse regole di NumPy(che sono le stesse regole del prodotto tra matrici in generale). 

7. La riga `tensor_3 = tf.reshape(tensor_2, shape=(2,3))` ridimensiona il tensore `tensor_2` in una nuova forma 2x3 e lo assegna alla variabile `tensor_3`.
8. La riga `trasposta_tensor_3 = tf.transpose(tensor_3)` calcola la trasposta del tensore `tensor_3` e la assegna alla variabile `trasposta_tensor_3`.
9. La riga `print(tensor_3, trasposta_tensor_3)` stampa il valore dei tensori `tensor_3` e `trasposta_tensor_3`.
10. La riga `z = tf.matmul(tensor_2, tensor_3)` esegue una moltiplicazione di matrici tra `tensor_2` e `tensor_3` utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `z`.
11. La riga `print(z)` stampa il valore del tensore `z`, che è il risultato della moltiplicazione di matrici.
12. La riga `za= tf.tensordot(tensor_2, tensor_3, axes= 2)` esegue un'operazione di tensordot tra `tensor_2` e `tensor_1` con una contrazione lungo 2 assi. Il risultato è un singolo scalare che viene assegnato alla variabile `za`.
13. La riga `print(za)` stampa il valore del tensore `za`, che è il risultato dell'operazione tensordot.
