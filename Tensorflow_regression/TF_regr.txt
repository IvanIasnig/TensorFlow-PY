------------------------------LINEAR REGRESSION----------------------------------------------

La regressione lineare è un metodo statistico che permette di esaminare la relazione tra due o più variabili di interesse. Mentre una regressione lineare semplice esamina la relazione tra due variabili (una variabile indipendente e una variabile dipendente), una regressione lineare multipla esamina la relazione tra tre o più variabili (una o più variabili indipendenti e una variabile dipendente).
Es: var. dipendente -> prezzo casa var. indipendente-> numero di stanze della casa, mq ecc... 
Si può usare l'one-hot per gli inputes: [[0,0,0,1], ->ha 3 stanze
                                         [0,1,0,0], -> ha 1 bagno
                                         [1,0,0,0]] -> ha 0 garage

L'obiettivo della regressione lineare è trovare la linea di migliore adattamento attraverso i dati. Questa linea è chiamata linea di regressione.

Nel contesto di regressione lineare, il termine "lineare" si riferisce al fatto che la relazione tra le variabili indipendenti e la variabile dipendente può essere espressa come una somma pesata delle variabili indipendenti, più un termine di errore. La relazione può essere espressa con la seguente formula:

y = a + b*X + e

Dove:
- y è la variabile dipendente (o "risposta")
- X è la variabile indipendente (o "caratteristica", "predittore" o "input")
- a è l'intercetta (o "coefficiente di regressione lineare")
- b è la pendenza (o "coefficiente di regressione")
- e è l'errore (o "residuo")

L'intercetta, a, rappresenta il valore previsto di y quando tutte le variabili indipendenti (X) sono uguali a zero. La pendenza, b, rappresenta il cambiamento previsto in y per un aumento di una unità in X. L'errore, e, rappresenta la differenza tra il valore effettivo e il valore previsto di y.

La regressione lineare può essere utilizzata per predire valori futuri, ma deve essere usata con cautela quando si tratta di estendere le previsioni al di fuori dell'intervallo dei dati osservati.

------------------------------------- TYPICAL DATA -----------------------------------------------------

HYPERPARAMETER                         TYPICAL VALUE 
                                                                            
Input layer shape                      Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction)
                                                                           
Hidden layer(s)                        Problem specific, minimum = 1, maximum = unlimited      

Neurons per hidden layer               Problem specific, generally 10 to 100

Output layer shape                     Same shape as desired prediction shape (e.g. 1 for house price)

Hidden activation                      Usually ReLU (rectified linear unit)

Output activation                      None, ReLU, logistic/tanh

Loss function                          MSE (mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers

Optimizer                              SGD (stochastic gradient descent), Adam

----------------------creating sample regression data---------------------------------

Primo passo nella creazione di un modello di regressione lineare. Ecco un riassunto di ciò che fa:

1. Importa le librerie necessarie: TensorFlow per la creazione del modello di machine learning, numpy per le operazioni sui dati e matplotlib per la visualizzazione dei dati.
2. Crea un set di dati di esempio per la regressione lineare. `X` è la variabile indipendente e `y` è la variabile dipendente. In questo caso, i dati sono lineari e sembrano seguire la relazione y = X + 10.
3. Visualizza i dati utilizzando un grafico a dispersione (`scatter plot`). Questo serve per avere un'idea di come sono distribuiti i dati e per confermare che sembrano seguire una relazione lineare.
4. Esegue `plt.show()` per mostrare effettivamente il grafico a dispersione.
5. Crea due tensori TensorFlow `house_info` e `house_price`. Questi rappresentano un esempio di input e output per un modello di regressione. Nel contesto della predizione del prezzo delle case, potresti avere informazioni sulla casa come il numero di camere da letto, bagni e garage come input (`house_info`), e il prezzo della casa come output (`house_price`).

Tuttavia, questo codice non crea ancora un modello di regressione lineare. Sarebbero necessari ulteriori passaggi, come la creazione del modello utilizzando `tf.keras`, l'addestramento del modello sui dati e la valutazione delle sue prestazioni.

-------------------------------first regr-----------------------------------------------

```
X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])
```
Questa riga crea un tensore TensorFlow `X` che contiene i dati delle features, ovvero le variabili di ingresso del nostro modello. I tensori sono un concetto chiave in TensorFlow e rappresentano 
una generalizzazione dei vettori e matrici a più dimensioni.

```
y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])
```
Questa riga crea un altro tensore `y` che contiene le etichette (labels) o i target, ovvero i valori di uscita che il nostro modello dovrebbe prevedere.

```
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])
```
Questa parte del codice crea il modello di rete neurale utilizzando l'API Sequential di Keras. Il modello è composto da un solo strato, un Dense layer, che è lo strato più comune in una rete neurale. 
Un Dense layer è un layer di neuroni completamente connessi, cioè ogni neurone in uno strato è connesso a tutti i neuroni nello strato precedente.
Il parametro `1` indica che ci sarà un solo neurone in questo strato, che è comune in modelli di regressione lineare.

```
model.compile(loss=tf.keras.losses.mae, 
              optimizer=tf.keras.optimizers.SGD(), 
              metrics=["mae"])
```
La funzione `compile` configura il processo di apprendimento del modello. Si specificano tre cose: la funzione di perdita, l'ottimizzatore e le metriche. 
La funzione di perdita (loss function) è la funzione che il modello cercherà di minimizzare durante l'addestramento. In questo caso, si utilizza l'errore medio assoluto (MAE - Mean Absolute Error).
L'ottimizzatore determina come il modello si aggiorna e apprende dai suoi dati. SGD (Stochastic Gradient Descent) è un metodo comune per ottimizzare i modelli di apprendimento automatico, e in particolare le reti neurali.
Le metriche sono utilizzate per monitorare le fasi di addestramento e test. In questo caso, si utilizza anche MAE come metrica.

```
model.fit(tf.expand_dims(X, axis=-1), y, epochs=1000)
```
La funzione `fit` è dove avviene effettivamente l'addestramento del modello. Gli argomenti sono le features e le etichette (X e y), e il numero di epoche, che è il numero di volte che il modello 
attraverserà l'intero dataset. 

In questo caso, l'addestramento viene eseguito per 1000 epoche. Il comando `tf.expand_dims(X, axis=-1)` viene utilizzato per modificare la forma del tensore `X` aggiungendo una dimensione extra, 
necessaria per l'addestramento del modello. Questo perché la funzione `fit` richiede che le features di ingresso siano un tensore 2D, e attualmente `X` è un tensore 1D.

```
output = model.predict([17.0])
print(output)
```
Infine, una volta che il modello è stato addestrato, possiamo utilizzarlo per fare previsioni su nuovi dati. In questo caso, stiamo cercando di prevedere l'output corrispondente al valore di input 17.0. Il risultato della previsione viene poi stampato a schermo.

In sintesi, il codice sta creando un modello di rete neurale semplice per eseguire una regressione lineare su un set di dati. Il modello viene poi addestrato su quei dati e utilizzato per fare una previsione.

--------------------- Steps in improving a model with TensorFlow 1/2/3 -----------------------

Il secondo codice che hai fornito è un esempio di un modello di rete neurale profonda, in contrasto con il primo codice che è un modello di regressione lineare semplice. Qui ci sono alcune delle differenze chiave tra i due codici:

1. **Struttura del modello**: Il secondo codice utilizza un modello di rete neurale profonda, con tre strati nascosti, ciascuno con 100 neuroni. Questo è evidente dalla sequenza di strati nel modello: 
    ```
    model = tf.keras.Sequential([
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(1)
    ])
    ```
    Ogni strato nascosto utilizza la funzione di attivazione ReLU (Rectified Linear Unit), che è una funzione comune per le reti neurali. L'ultimo strato ha un solo neurone, che emette il risultato finale. 
2. **Ottimizzatore**: Il secondo codice utilizza l'ottimizzatore Adam invece dello Stochastic Gradient Descent (SGD) utilizzato nel primo codice. Adam è un ottimizzatore più avanzato che utilizza un tasso di apprendimento adattivo. Inoltre, il codice specifica un tasso di apprendimento (`lr`) di 0.0001 per l'ottimizzatore Adam.
3. **Epoca**: L'ultimo punto principale è che nel secondo codice, il modello viene allenato per 5000 epoche, rispetto a 1000 nel primo codice. Questo può essere dovuto al fatto che le reti neurali profonde possono richiedere più tempo per convergere o per imparare i dati.
In sintesi, il secondo codice è più complesso e potrebbe avere la capacità di modellare relazioni più complesse nei dati grazie all'uso di una rete neurale profonda. Tuttavia, avrà bisogno di più tempo per l'addestramento rispetto al primo codice a causa del numero maggiore di parametri e del numero più alto di epoche.

Spesso si spacchettano i modelli di AI in modo da andarte a verificare il corretto funzionamento della rete.

Improving a model
How do you think you'd improve upon our current model?

If you guessed by tweaking some of the things we did above, you'd be correct.

To improve our model, we alter almost every part of the 3 steps we went through before.

MIGLIORAMENTO DI UN MODELLO:

1. Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.
2. Compiling a model - you might want to choose optimization function or perhaps change the learning rate of the optimization function.
3. Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer) or on more data (give the model more examples to learn from).

Nell'ultimo esempio di questa serie di 3 snippet di codice riguardo a "improving a model" si vede che cambiando i parametri forniti riceviamo un output più accurato addirittura riducendo a 
1/10 il numero delle epoche (siamo passati dalle 1000  del codice sopra a 100). Di base siamo andati a tarare in maniera migliore il codice usando i suggerimenti dell'elenco qui sopra.

-------------------Evaluating a TensorFlow model part 1 -4 -----------------------

When it comes to evaluation, you'll want to remember the words: "visualize, visualize, visualize."
This is because you're probably better looking at something (doing) than you are thinking about something.
It's a good idea to visualize:

The data - what data are you working with? What does it look like?
The model itself - what does the architecture look like? What are the different shapes?
The training of a model - how does a model perform while it learns?
The predictions of a model - how do the predictions of a model line up against the ground truth (the original labels)?
Let's start by visualizing the model.

---

Split data into training/test set
One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).

Each set serves a specific purpose:

Training set - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester).
Validation set - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam).
Test set - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).


1. `X = tf.range(-100, 100, 4)`: Questo comando crea un tensore con una serie di valori che inizia da -100, finisce a 100 e con uno step di 4.
2. `y = X + 10`: Questo crea un nuovo tensore `y` che è semplicemente il tensore `X` con 10 aggiunto a ciascun elemento.
3. `X_train = X[:40], y_train = y[:40]`: Queste righe dividono i tuoi dati originali nei set di training. Prendono i primi 40 elementi di `X` e `y`.
4. `X_test = X[40:], y_test = y[40:]`: Queste righe dividono i tuoi dati originali nei set di test. Prendono gli elementi di `X` e `y` dall'indice 40 in poi.
5. Creazione del modello con `tf.keras.Sequential`: Questo crea un modello sequenziale con due layer. Il primo layer ha 10 neuroni ed accetta un input di shape [1]. Il secondo layer è il layer di output e ha un singolo neurone.
6. `model.compile(...)`: Questa funzione compila il modello. Significa che stai definendo la funzione di perdita, l'ottimizzatore e le metriche che vuoi utilizzare. La funzione di perdita è la funzione che il tuo modello cercherà di minimizzare durante l'addestramento. L'ottimizzatore è l'algoritmo che il tuo modello utilizzerà per minimizzare la funzione di perdita. Le metriche sono le funzioni utilizzate per giudicare le prestazioni del tuo modello.
   - `loss=tf.keras.losses.mae`: Qui stai utilizzando Mean Absolute Error (MAE) come funzione di perdita.
   - `optimizer=tf.keras.optimizers.SGD()`: Stai utilizzando il Stochastic Gradient Descent (SGD) come ottimizzatore.
   - `metrics=["mae"]`: Vuoi che il tuo modello monitori MAE come metrica durante l'addestramento.
7. `model.summary()`: Questa riga stampa un riepilogo del tuo modello. Mostra i layer del modello, la forma dell'output di ciascun layer, e il numero di parametri (pesi e bias) in ciascun layer.
8. `model.fit(X_train, y_train, epochs=100, verbose=1)`: Questa riga addestra il modello sui dati di addestramento per 100 epoche. Un'epoca è un singolo passaggio attraverso l'intero set di dati di addestramento.
9. `plot_model(model, show_shapes=True)`: Questa funzione di keras genera una rappresentazione grafica del modello. Mostra i layer del modello, le loro forme di output e mostra anche i nomi dei layer. Il parametro `show_shapes=True` indica alla funzione di mostrare le forme di input e output di ogni layer.

-------------------------Evaluating a TensorFlow model part 5-----------------------

Stesso codice di prima, riprendiamo dal y_pred = model.predict(X_test)

risultato trovato -> y_pred 
[[ 67.01532 ]
 [ 71.36802 ]
 [ 75.72072 ]
 [ 80.07342 ]
 [ 84.426125]
 [ 88.778824]
 [ 93.13152 ]
 [ 97.48424 ]
 [101.83692 ]
 [106.18962 ]]

risultato atteso -> y_test
tf.Tensor([ 70  74  78  82  86  90  94  98 102 106], shape=(10,), dtype=int32)

🔑 Note: If you think you're going to be visualizing something a lot, it's a good idea to functionize it so you can use it later.

def plot_predictions(train_data=X_train, 
                     train_labels=y_train, 
                     test_data=X_test, 
                     test_labels=y_test, 
                     predictions=y_preds):

Purtroppo, credo a caausa di un cambiamento nella sintassi non mi funzionava la sintassi del plot_predicctions qui sopra.
Quindi sono andato a rifare il grafico di mat_plot usando i dati originari, in verde abbiamo la parte del test, in rosso i valori che ci vengono fuori.
Ovviamente in parole povere la distanza tra i due raprresente l'errore riscontrato nel nostro grafico.

Nei 3 video successivi mostra la differenza tra il mae il mse, calcolandoli separatamente (il codice resta sempre -> tf.keras.losses.mae() ).
Questi video non mi sembvravano importanti, l'unica cosa importante è (come al solito) fare attenzione alla shape dei tensori.
Potevo scriverli anche entrambi con questa sintassi: tf.keras.losses.mae

-------------------------- Setting up TensorFlow modelling experiments part 1 (start with a simple model)-----------------

Running experiments to improve a model
After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.

Again, there are many different ways you can do this, but 3 of the main ones are:

Get more data - get more examples for your model to train on (more opportunities to learn patterns).
Make your model larger (use a more complex model) - this might come in the form of more layers or more hidden units in each layer.
Train for longer - give your model more of a chance to find the patterns in the data.

------------------------- Setting up TensorFlow modelling experiments part 2 (increasing complexity)------------------------------

Aggunto un'hidden layer prima dell'ouput layer : tf.keras.layers.Dense(64, activation= "relu").
Aggiunte epoches (sempre stare attenti a non aggiungerne troppe sennò il modello va in overfitting, quindi impara il modello ma non lo generalizza)

Gli hidden layers in una rete neurale hanno la funzione di apprendere rappresentazioni astratte dei dati di input. Sono chiamati "hidden" perché non sono né input né output: lavorano in background per trasformare i dati in modi che permettono alla rete di fare previsioni accurate.
Ogni hidden layer in una rete neurale è composto da un numero di nodi o "neuroni". Ogni nodo prende un insieme di input, esegue una somma pesata di questi input, applica una funzione di attivazione a questa somma e quindi passa il risultato agli nodi nel layer successivo. 
Ad esempio, in una rete neurale completamente connessa o "densa", ogni nodo nel primo hidden layer prende l'intero set di input, esegue la sua somma pesata, applica la sua funzione di attivazione e passa il risultato a ogni nodo nel secondo hidden layer. Questo processo si ripete attraverso ogni hidden layer fino a raggiungere l'output layer.
Gli hidden layers permettono alla rete neurale di apprendere rappresentazioni complesse dei dati. Con un singolo hidden layer, una rete neurale può apprendere qualsiasi funzione che separa i dati in modo lineare. Ma aggiungendo più hidden layers, la rete può apprendere funzioni che separano i dati in modo non lineare. Questo è importante perché molti problemi del mondo reale sono non lineari.
Ad esempio, immagina di avere un insieme di immagini di gatti e cani e vuoi che la tua rete neurale classifichi le immagini. Gli input alla tua rete sarebbero i pixel delle immagini. Il primo hidden layer potrebbe apprendere a riconoscere caratteristiche semplici come linee e colori. Il secondo hidden layer potrebbe costruire su questo per riconoscere forme più complesse come occhi e nasi. Il terzo hidden layer potrebbe riconoscere intere facce di gatti e cani.
 Infine, l'output layer prenderebbe queste rappresentazioni ad alto livello delle immagini e farebbe la sua previsione: gatto o cane.
Pertanto, gli hidden layers sono fondamentali per l'apprendimento profondo e l'apprendimento di rappresentazioni complesse dei dati.

------------------------ Comparing and tracking your TensorFlow modelling experiments--------------

🔑 Note: One of your main goals should be to minimize the time between your experiments. 
The more experiments you do, the more things you'll figure out which don't work and in turn, 
get closer to figuring out what does work. Remember the machine learning practitioner's motto:
 "experiment, experiment, experiment".

We've done a simple version of this above (keeping the results in different variables).

📖 Resource: But as you build more models, you'll want to look into using tools such as:
TensorBoard - a component of the TensorFlow library to help track modelling experiments (we'll see this later).
Weights & Biases - a tool for tracking all kinds of machine learning experiments (the good news for Weights & Biases is it plugs into TensorBoard).

---------------------- How to save a TensorFlow model ---------------------------------



1. `model_2.save('prova_salvataggio_modello')`: Questo comando salva il tuo modello nel formato SavedModel di TensorFlow. SavedModel è un formato che include sia la struttura del modello
(la sua architettura) che i suoi pesi. È un formato molto flessibile che ti permette di salvare modelli interi, così come parti di modelli o persino computation graphs. 
Può essere utilizzato sia con TensorFlow che con TensorFlow Serving, il che lo rende utile se si prevede di distribuire il modello in produzione.

2. `model_2.save('prova_salvataggio_modello.h5')`: Questo comando salva il tuo modello nel formato HDF5, che è un formato di dati comune per le grandi matrici numeriche,
come i pesi di un modello di apprendimento automatico. Quando salvi un modello Keras nel formato HDF5, vengono salvati sia la struttura del modello che i suoi pesi.
Anche se questo formato è meno flessibile del formato SavedModel (ad esempio, non può salvare computation graphs personalizzati), può essere più semplice da utilizzare in alcuni casi,
in particolare se stai lavorando esclusivamente con Keras e non hai bisogno delle funzionalità extra di TensorFlow.

(Nella lezione 61 mostra come salvare e scaricare un modello da Google Colab)


Ecco un riassunto dettagliato del tuo codice:

1. Iniziamo con l'importazione di pandas e l'impostazione di `display.max_columns` su `None` per visualizzare tutte le colonne di un DataFrame quando viene stampato:

   ```
   import pandas as pd
   pd.set_option('display.max_columns', None)
   ```

2. Successivamente, leggiamo un file CSV da un URL utilizzando `pd.read_csv()`. Il file contiene dati sull'assicurazione sanitaria:

   ```
   insurance = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
   ```

3. Utilizziamo l'one-hot encoding per convertire le variabili categoriche in una forma che possiamo dare al modello di machine learning. Questo viene fatto utilizzando la funzione `pd.get_dummies()`:

   ```
   insurance_one_hot = pd.get_dummies(insurance)
   ```

4. Successivamente, separiamo le feature (X) e la label (y). Le feature sono tutte le colonne eccetto "charges", che è la label che stiamo cercando di prevedere:

   ```
   X = insurance_one_hot.drop("charges", axis=1)
   y = insurance_one_hot["charges"]
   ```

5. Ora suddividiamo i dati in un set di training e un set di test usando la funzione `train_test_split()` di scikit-learn. Il 80% dei dati sarà utilizzato per l'addestramento e il 20% per il test:

   ```
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   ```

6. Poi impostiamo il seme per il generatore di numeri casuali di TensorFlow per garantire che i risultati siano riproducibili:

   ```
   tf.random.set_seed(42)
   ```

7. Creiamo il modello di rete neurale utilizzando l'API Sequenziale di TensorFlow. Il modello consiste in tre livelli:

   - Un livello nascosto con 100 unità.
   - Un altro livello nascosto con 10 unità.
   - Un livello di output con 1 unità (per prevedere la "charge").

   ```
   insurance_model_2 = tf.keras.Sequential([
     tf.keras.layers.Dense(100), 
     tf.keras.layers.Dense(10), 
     tf.keras.layers.Dense(1) 
   ])
   ```

8. Compiliamo il modello con la loss Mean Absolute Error (MAE), l'ottimizzatore Adam, e impostiamo MAE come metrica per il monitoraggio durante l'addestramento:

   ```
   insurance_model_2.compile(loss=tf.keras.losses.mae,
                             optimizer=tf.keras.optimizers.Adam(), 
                             metrics=['mae'])
   ```

9. Addestriamo il modello per 115 epoche e salviamo la storia dell'addestramento per una successiva visualizzazione:

   ```
   history = insurance_model_2.fit(X_train, y_train, epochs=115)
   ```

10. Valutiamo il modello sul set di test:

    ```
    print(insurance_model_2.evaluate(X_test, y_test))
    ```

11. Infine, tracciamo la curva di perdita (loss curve) per visualizzare come la perdita del modello sia cambiata durante l'addestramento. Creiamo un DataFrame da `history.history`, quindi utilizziamo la funzione `plot()` di pandas. Impostiamo anche le etichette degli assi x e y:

    ```
    pd.DataFrame(history.history).plot()
    plt.ylabel("loss")
    plt.xlabel("epochs");
    plt.show() 
    ```

Questo plot può aiutarti a capire se il tuo modello soffre di overfitting o underfitting. Se la perdita sui dati di training continua a diminuire mentre la perdita sui dati di validazione inizia ad aumentare, il modello sta overfittando i dati di training. D'altro canto, se entrambe le perdite continuano a diminuire senza divergere, allora il modello potrebbe beneficiare di ulteriori epoche di addestramento.
In sintesi, questo codice legge un dataset sull'assicurazione sanitaria, prepara i dati per l'addestramento, addestra una rete neurale per prevedere le spese di assicurazione sulla base di varie feature, valuta il modello e poi visualizza la curva di perdita.

------------------------ Preprocessing data (normalization and standardization) ------------------------

A common practice when working with neural networks is to make sure all of the data you pass to them is in the range 0 to 1.

This practice is called normalization (scaling all values from their original range to, e.g. between 0 and 100,000 to be between 0 and 1).

There is another process call standardization which converts all of your data to unit variance and 0 mean.

These two practices are often part of a preprocessing pipeline (a series of functions to prepare your data for use with neural networks).

Knowing this, some of the major steps you'll take to preprocess your data for a neural network include:

Turning all of your data to numbers (a neural network can't handle strings).
Making sure your data is in the right shape (verifying input and output shapes).
Feature scaling:
Normalizing data (making sure all values are between 0 and 1). This is done by subtracting the minimum value then dividing by the maximum value minus the minimum. This is also referred to as min-max scaling.
Standardization (making sure all values have a mean of 0 and a variance of 1). This is done by subtracting the mean value from the target feature and then dividing it by the standard deviation.
Which one should you use?
With neural networks you'll tend to favour normalization as they tend to prefer values between 0 and 1 (you'll see this espcially with image processing), however, you'll often find a neural network can perform pretty well with minimal feature scaling.
📖 Resource: For more on preprocessing data, I'd recommend reading the following resources:

Scikit-Learn's documentation on preprocessing data.
Scale, Standardize or Normalize with Scikit-Learn by Jeff Hale.
We've already turned our data into numbers using get_dummies(), let's see how we'd normalize it as well.

RICOSTRUITO IL MODELLO NORMALIZZATO ->
normalizzando il risultato il modello diventa più accurato, dandoci (con gli stessi parametri) una loss e mae molto minore.