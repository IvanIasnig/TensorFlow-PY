------------------------------LINEAR REGRESSION----------------------------------------------

La regressione lineare è un metodo statistico che permette di esaminare la relazione tra due o più variabili di interesse. Mentre una regressione lineare semplice esamina la relazione tra due variabili (una variabile indipendente e una variabile dipendente), una regressione lineare multipla esamina la relazione tra tre o più variabili (una o più variabili indipendenti e una variabile dipendente).
Es: var. dipendente -> prezzo casa var. indipendente-> numero di stanze della casa, mq ecc... 
Si può usare l'one-hot per gli inputes: [[0,0,0,1], ->ha 3 stanze
                                         [0,1,0,0], -> ha 1 bagno
                                         [1,0,0,0]] -> ha 0 garage

L'obiettivo della regressione lineare è trovare la linea di migliore adattamento attraverso i dati. Questa linea è chiamata linea di regressione.

Nel contesto di regressione lineare, il termine "lineare" si riferisce al fatto che la relazione tra le variabili indipendenti e la variabile dipendente può essere espressa come una somma pesata delle variabili indipendenti, più un termine di errore. La relazione può essere espressa con la seguente formula:

y = a + b*X + e

Dove:
- y è la variabile dipendente (o "risposta")
- X è la variabile indipendente (o "caratteristica", "predittore" o "input")
- a è l'intercetta (o "coefficiente di regressione lineare")
- b è la pendenza (o "coefficiente di regressione")
- e è l'errore (o "residuo")

L'intercetta, a, rappresenta il valore previsto di y quando tutte le variabili indipendenti (X) sono uguali a zero. La pendenza, b, rappresenta il cambiamento previsto in y per un aumento di una unità in X. L'errore, e, rappresenta la differenza tra il valore effettivo e il valore previsto di y.

La regressione lineare può essere utilizzata per predire valori futuri, ma deve essere usata con cautela quando si tratta di estendere le previsioni al di fuori dell'intervallo dei dati osservati.

------------------------------------- TYPICAL DATA -----------------------------------------------------

HYPERPARAMETER                         TYPICAL VALUE 
                                                                            
Input layer shape                      Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction)
                                                                           
Hidden layer(s)                        Problem specific, minimum = 1, maximum = unlimited      

Neurons per hidden layer               Problem specific, generally 10 to 100

Output layer shape                     Same shape as desired prediction shape (e.g. 1 for house price)

Hidden activation                      Usually ReLU (rectified linear unit)

Output activation                      None, ReLU, logistic/tanh

Loss function                          MSE (mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers

Optimizer                              SGD (stochastic gradient descent), Adam

----------------------creating sample regression data---------------------------------

Primo passo nella creazione di un modello di regressione lineare. Ecco un riassunto di ciò che fa:

1. Importa le librerie necessarie: TensorFlow per la creazione del modello di machine learning, numpy per le operazioni sui dati e matplotlib per la visualizzazione dei dati.
2. Crea un set di dati di esempio per la regressione lineare. `X` è la variabile indipendente e `y` è la variabile dipendente. In questo caso, i dati sono lineari e sembrano seguire la relazione y = X + 10.
3. Visualizza i dati utilizzando un grafico a dispersione (`scatter plot`). Questo serve per avere un'idea di come sono distribuiti i dati e per confermare che sembrano seguire una relazione lineare.
4. Esegue `plt.show()` per mostrare effettivamente il grafico a dispersione.
5. Crea due tensori TensorFlow `house_info` e `house_price`. Questi rappresentano un esempio di input e output per un modello di regressione. Nel contesto della predizione del prezzo delle case, potresti avere informazioni sulla casa come il numero di camere da letto, bagni e garage come input (`house_info`), e il prezzo della casa come output (`house_price`).

Tuttavia, questo codice non crea ancora un modello di regressione lineare. Sarebbero necessari ulteriori passaggi, come la creazione del modello utilizzando `tf.keras`, l'addestramento del modello sui dati e la valutazione delle sue prestazioni.

-------------------------------first regr-----------------------------------------------

```
X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])
```
Questa riga crea un tensore TensorFlow `X` che contiene i dati delle features, ovvero le variabili di ingresso del nostro modello. I tensori sono un concetto chiave in TensorFlow e rappresentano 
una generalizzazione dei vettori e matrici a più dimensioni.

```
y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])
```
Questa riga crea un altro tensore `y` che contiene le etichette (labels) o i target, ovvero i valori di uscita che il nostro modello dovrebbe prevedere.

```
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1)
])
```
Questa parte del codice crea il modello di rete neurale utilizzando l'API Sequential di Keras. Il modello è composto da un solo strato, un Dense layer, che è lo strato più comune in una rete neurale. 
Un Dense layer è un layer di neuroni completamente connessi, cioè ogni neurone in uno strato è connesso a tutti i neuroni nello strato precedente.
Il parametro `1` indica che ci sarà un solo neurone in questo strato, che è comune in modelli di regressione lineare.

```
model.compile(loss=tf.keras.losses.mae, 
              optimizer=tf.keras.optimizers.SGD(), 
              metrics=["mae"])
```
La funzione `compile` configura il processo di apprendimento del modello. Si specificano tre cose: la funzione di perdita, l'ottimizzatore e le metriche. 
La funzione di perdita (loss function) è la funzione che il modello cercherà di minimizzare durante l'addestramento. In questo caso, si utilizza l'errore medio assoluto (MAE - Mean Absolute Error).
L'ottimizzatore determina come il modello si aggiorna e apprende dai suoi dati. SGD (Stochastic Gradient Descent) è un metodo comune per ottimizzare i modelli di apprendimento automatico, e in particolare le reti neurali.
Le metriche sono utilizzate per monitorare le fasi di addestramento e test. In questo caso, si utilizza anche MAE come metrica.

```
model.fit(tf.expand_dims(X, axis=-1), y, epochs=1000)
```
La funzione `fit` è dove avviene effettivamente l'addestramento del modello. Gli argomenti sono le features e le etichette (X e y), e il numero di epoche, che è il numero di volte che il modello 
attraverserà l'intero dataset. 

In questo caso, l'addestramento viene eseguito per 1000 epoche. Il comando `tf.expand_dims(X, axis=-1)` viene utilizzato per modificare la forma del tensore `X` aggiungendo una dimensione extra, 
necessaria per l'addestramento del modello. Questo perché la funzione `fit` richiede che le features di ingresso siano un tensore 2D, e attualmente `X` è un tensore 1D.

```
output = model.predict([17.0])
print(output)
```
Infine, una volta che il modello è stato addestrato, possiamo utilizzarlo per fare previsioni su nuovi dati. In questo caso, stiamo cercando di prevedere l'output corrispondente al valore di input 17.0. Il risultato della previsione viene poi stampato a schermo.

In sintesi, il codice sta creando un modello di rete neurale semplice per eseguire una regressione lineare su un set di dati. Il modello viene poi addestrato su quei dati e utilizzato per fare una previsione.

--------------------- Steps in improving a model with TensorFlow 1/2/3 -----------------------

Il secondo codice che hai fornito è un esempio di un modello di rete neurale profonda, in contrasto con il primo codice che è un modello di regressione lineare semplice. Qui ci sono alcune delle differenze chiave tra i due codici:

1. **Struttura del modello**: Il secondo codice utilizza un modello di rete neurale profonda, con tre strati nascosti, ciascuno con 100 neuroni. Questo è evidente dalla sequenza di strati nel modello: 
    ```
    model = tf.keras.Sequential([
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(100, activation="relu"),
      tf.keras.layers.Dense(1)
    ])
    ```
    Ogni strato nascosto utilizza la funzione di attivazione ReLU (Rectified Linear Unit), che è una funzione comune per le reti neurali. L'ultimo strato ha un solo neurone, che emette il risultato finale. 
2. **Ottimizzatore**: Il secondo codice utilizza l'ottimizzatore Adam invece dello Stochastic Gradient Descent (SGD) utilizzato nel primo codice. Adam è un ottimizzatore più avanzato che utilizza un tasso di apprendimento adattivo. Inoltre, il codice specifica un tasso di apprendimento (`lr`) di 0.0001 per l'ottimizzatore Adam.
3. **Epoca**: L'ultimo punto principale è che nel secondo codice, il modello viene allenato per 5000 epoche, rispetto a 1000 nel primo codice. Questo può essere dovuto al fatto che le reti neurali profonde possono richiedere più tempo per convergere o per imparare i dati.
In sintesi, il secondo codice è più complesso e potrebbe avere la capacità di modellare relazioni più complesse nei dati grazie all'uso di una rete neurale profonda. Tuttavia, avrà bisogno di più tempo per l'addestramento rispetto al primo codice a causa del numero maggiore di parametri e del numero più alto di epoche.

Spesso si spacchettano i modelli di AI in modo da andarte a verificare il corretto funzionamento della rete.

Improving a model
How do you think you'd improve upon our current model?

If you guessed by tweaking some of the things we did above, you'd be correct.

To improve our model, we alter almost every part of the 3 steps we went through before.

MIGLIORAMENTO DI UN MODELLO:

1. Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.
2. Compiling a model - you might want to choose optimization function or perhaps change the learning rate of the optimization function.
3. Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer) or on more data (give the model more examples to learn from).

Nell'ultimo esempio di questa serie di 3 snippet di codice riguardo a "improving a model" si vede che cambiando i parametri forniti riceviamo un output più accurato addirittura riducendo a 
1/10 il numero delle epoche (siamo passati dalle 1000  del codice sopra a 100). Di base siamo andati a tarare in maniera migliore il codice usando i suggerimenti dell'elenco qui sopra.

-------------------Evaluating a TensorFlow model part 1 -4 -----------------------

When it comes to evaluation, you'll want to remember the words: "visualize, visualize, visualize."
This is because you're probably better looking at something (doing) than you are thinking about something.
It's a good idea to visualize:

The data - what data are you working with? What does it look like?
The model itself - what does the architecture look like? What are the different shapes?
The training of a model - how does a model perform while it learns?
The predictions of a model - how do the predictions of a model line up against the ground truth (the original labels)?
Let's start by visualizing the model.

---

Split data into training/test set
One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).

Each set serves a specific purpose:

Training set - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester).
Validation set - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam).
Test set - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).


1. `X = tf.range(-100, 100, 4)`: Questo comando crea un tensore con una serie di valori che inizia da -100, finisce a 100 e con uno step di 4.
2. `y = X + 10`: Questo crea un nuovo tensore `y` che è semplicemente il tensore `X` con 10 aggiunto a ciascun elemento.
3. `X_train = X[:40], y_train = y[:40]`: Queste righe dividono i tuoi dati originali nei set di training. Prendono i primi 40 elementi di `X` e `y`.
4. `X_test = X[40:], y_test = y[40:]`: Queste righe dividono i tuoi dati originali nei set di test. Prendono gli elementi di `X` e `y` dall'indice 40 in poi.
5. Creazione del modello con `tf.keras.Sequential`: Questo crea un modello sequenziale con due layer. Il primo layer ha 10 neuroni ed accetta un input di shape [1]. Il secondo layer è il layer di output e ha un singolo neurone.
6. `model.compile(...)`: Questa funzione compila il modello. Significa che stai definendo la funzione di perdita, l'ottimizzatore e le metriche che vuoi utilizzare. La funzione di perdita è la funzione che il tuo modello cercherà di minimizzare durante l'addestramento. L'ottimizzatore è l'algoritmo che il tuo modello utilizzerà per minimizzare la funzione di perdita. Le metriche sono le funzioni utilizzate per giudicare le prestazioni del tuo modello.
   - `loss=tf.keras.losses.mae`: Qui stai utilizzando Mean Absolute Error (MAE) come funzione di perdita.
   - `optimizer=tf.keras.optimizers.SGD()`: Stai utilizzando il Stochastic Gradient Descent (SGD) come ottimizzatore.
   - `metrics=["mae"]`: Vuoi che il tuo modello monitori MAE come metrica durante l'addestramento.
7. `model.summary()`: Questa riga stampa un riepilogo del tuo modello. Mostra i layer del modello, la forma dell'output di ciascun layer, e il numero di parametri (pesi e bias) in ciascun layer.
8. `model.fit(X_train, y_train, epochs=100, verbose=1)`: Questa riga addestra il modello sui dati di addestramento per 100 epoche. Un'epoca è un singolo passaggio attraverso l'intero set di dati di addestramento.
9. `plot_model(model, show_shapes=True)`: Questa funzione di keras genera una rappresentazione grafica del modello. Mostra i layer del modello, le loro forme di output e mostra anche i nomi dei layer. Il parametro `show_shapes=True` indica alla funzione di mostrare le forme di input e output di ogni layer.

-------------------------Evaluating a TensorFlow model part 5-----------------------

Stesso codice di prima, riprendiamo dal y_pred = model.predict(X_test)

risultato trovato -> y_pred 
[[ 67.01532 ]
 [ 71.36802 ]
 [ 75.72072 ]
 [ 80.07342 ]
 [ 84.426125]
 [ 88.778824]
 [ 93.13152 ]
 [ 97.48424 ]
 [101.83692 ]
 [106.18962 ]]

risultato atteso -> y_test
tf.Tensor([ 70  74  78  82  86  90  94  98 102 106], shape=(10,), dtype=int32)

🔑 Note: If you think you're going to be visualizing something a lot, it's a good idea to functionize it so you can use it later.

def plot_predictions(train_data=X_train, 
                     train_labels=y_train, 
                     test_data=X_test, 
                     test_labels=y_test, 
                     predictions=y_preds):

Purtroppo, credo a caausa di un cambiamento nella sintassi non mi funzionava la sintassi del plot_predicctions qui sopra.
Quindi sono andato a rifare il grafico di mat_plot usando i dati originari, in verde abbiamo la parte del test, in rosso i valori che ci vengono fuori.
Ovviamente in parole povere la distanza tra i due raprresente l'errore riscontrato nel nostro grafico.

Nei 3 video successivi mostra la differenza tra il mae il mse, calcolandoli separatamente (il codice resta sempre -> tf.keras.losses.mae() ).
Questi video non mi sembvravano importanti, l'unica cosa importante è (come al solito) fare attenzione alla shape dei tensori.
Potevo scriverli anche entrambi con questa sintassi: tf.keras.losses.mae

---------------------------Setting up TensorFlow modelling experiments part 1 (start with a simple model)-----------------

Running experiments to improve a model
After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.

Again, there are many different ways you can do this, but 3 of the main ones are:

Get more data - get more examples for your model to train on (more opportunities to learn patterns).
Make your model larger (use a more complex model) - this might come in the form of more layers or more hidden units in each layer.
Train for longer - give your model more of a chance to find the patterns in the data.