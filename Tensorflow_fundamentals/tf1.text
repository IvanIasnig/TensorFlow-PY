output: Di seguito è riportata una breve descrizione degli elementi del log:

TensorFlow sta aprendo con successo le librerie dinamiche necessarie per funzionare con DirectML e DirectX 12 (dxgi.dll, d3d12.dll, directml.d6f03b303ac3c4f2eeb8ca631688c9757b361310.dll), che sono utilizzate per sfruttare le risorse della GPU per l'elaborazione.
TensorFlow rileva che il tuo sistema ha un'unità di elaborazione AMD Radeon RX 5700 e la seleziona come dispositivo DirectML.
TensorFlow ti informa che il binario è ottimizzato per utilizzare le istruzioni AVX e AVX2 per le operazioni critiche per le prestazioni. Se vuoi utilizzare queste istruzioni anche in altre operazioni, dovrai ricompilare TensorFlow con le flag del compilatore appropriate.
Non riesce a identificare il nodo NUMA della GPU, quindi utilizza il valore predefinito 0. Questo non dovrebbe causare problemi significativi.
In sintesi, questo output indica che TensorFlow si sta inizializzando correttamente e utilizza la tua GPU AMD Radeon RX 5700 per le operazioni. Non ci sono errori evidenti nel log.

NUMA (Non-Uniform Memory Access) è un'architettura di memoria utilizzata nei sistemi multiprocessore per consentire a ciascun processore di accedere direttamente alla propria memoria locale e alla memoria di altri processori in modo più efficiente. In un sistema NUMA, i nodi rappresentano unità di elaborazione (processori o gruppi di processori) e le loro memorie locali associate.
Nel contesto delle GPU, un nodo NUMA si riferisce alla memoria associata a una specifica GPU e al processore a cui è connessa. Identificare correttamente il nodo NUMA della GPU può essere utile per ottimizzare l'allocazione della memoria e la comunicazione tra la GPU e la CPU, migliorando le prestazioni delle applicazioni che utilizzano entrambe le risorse.
Nel log che hai condiviso, TensorFlow non è stato in grado di identificare il nodo NUMA associato alla tua GPU e ha utilizzato il valore predefinito 0. Sebbene questo possa non essere ottimale per le prestazioni, non dovrebbe causare malfunzionamenti o errori nel funzionamento del tuo programma. In alcuni casi, la mancata identificazione del nodo NUMA può essere dovuta al fatto che il kernel del sistema operativo non è stato compilato con il supporto NUMA abilitato.

-----------------------------------------------------------------------

Creazione di diversi tipi di tensori:
Il codice inizia importando il modulo tensorflow e creando diversi tipi di tensori:
- `scalar` è uno scalare, ovvero un singolo numero;
- `vector` è un vettore, ovvero un insieme di numeri che rappresenta una grandezza con direzione, come la velocità e la direzione del vento;
- `matrix` è una matrice, ovvero un array bidimensionale di numeri;
- `another_matrix` è un'altra matrice, ma in questo caso, i numeri sono in virgola mobile e il tipo di dato è specificato come `tf.float16`;
- `tensor` è un tensore tridimensionale, ovvero un array di numeri organizzato in tre dimensioni.

-------------------------Creating tensors-----------------------------

Tensori costanti e variabili:
- `changable_tensor` è un tensore variabile, ovvero un tensore i cui valori possono essere modificati;
- `unchangable_tensor` è un tensore costante, ovvero un tensore i cui valori non possono essere modificati.
Successivamente, il codice mostra come modificare un valore all'interno del tensore variabile utilizzando il metodo `assign()`. 
Infine, viene mostrato che il tentativo di modificare un tensore costante utilizzando lo stesso metodo non funziona, poiché i tensori costanti non possono essere modificati.
Raramente mi capiterà di dover scegliere tra i due, in ogni caso è meglio usare constant.

-------------------------Random tensors---------------------------------

è utile per effettuare una prima randomica calibrazione dei tensori.

Una distribuzione uniforme è un tipo di distribuzione di probabilità in cui tutti gli esiti possibili hanno la stessa probabilità di verificarsi. In altre parole, tutti gli eventi sono equiprobabili. La distribuzione uniforme può essere continua o discreta, a seconda del tipo di variabile a cui si applica.
1. Distribuzione uniforme continua: Quando la variabile di interesse è continua, la distribuzione uniforme si riferisce a un intervallo finito di valori reali, in cui la probabilità di ogni valore all'interno dell'intervallo è la stessa. Ad esempio, la distribuzione uniforme continua tra 0 e 1 indica che la probabilità di selezionare un numero casuale in questo intervallo è costante. La funzione densità di probabilità (PDF) di una distribuzione uniforme continua è una funzione costante su un intervallo specificato e zero al di fuori di esso.
2. Distribuzione uniforme discreta: Quando la variabile di interesse è discreta, la distribuzione uniforme si riferisce a un insieme finito di valori, in cui la probabilità di ogni valore è la stessa. Un esempio comune di distribuzione uniforme discreta è il lancio di un dado equilibrato a sei facce: ogni faccia ha la stessa probabilità di 1/6 di apparire.
La distribuzione uniforme è spesso utilizzata come modello semplice e di base nelle simulazioni e nei test di vari processi, poiché ogni evento ha la stessa probabilità di verificarsi e nessun evento è favorito rispetto agli altri.

Il codice crea due tensori con valori casuali utilizzando TensorFlow e il modulo `tf.random.Generator`. In entrambi i casi, viene utilizzato lo stesso seed (42), che garantisce che la sequenza di numeri casuali generata sia la stessa se vengono utilizzati gli stessi metodi di generazione.
1. Nel primo blocco di codice, viene creato un tensore chiamato `random_1`:
   - `tf.random.Generator.from_seed(42)` inizializza un generatore di numeri casuali con il seed 42;
   - `random_1.normal(shape=(3,2))` genera un tensore di forma (3,2) con valori casuali estratti da una distribuzione normale (anche detta gaussiana). Infine, il tensore `random_1` viene stampato.
2. Nel secondo blocco di codice, viene creato un tensore chiamato `random_2`:
   - `tf.random.Generator.from_seed(42)` inizializza un altro generatore di numeri casuali con lo stesso seed 42;
   - `random_2.uniform(shape=(3,2))` genera un tensore di forma (3,2) con valori casuali estratti da una distribuzione uniforme. Infine, il tensore `random_2` viene stampato.
Sebbene entrambi i tensori utilizzino lo stesso seed, la generazione dei numeri casuali è diversa poiché `random_1` utilizza una distribuzione normale, mentre `random_2` utilizza una distribuzione uniforme. Pertanto, i valori all'interno di questi due tensori saranno diversi.

----------------------Shuffle the order of elements in a tensor-------------

Utile per far in modo che l'AI sia addestrata in maniera equa.
Se ad esempio dovessi allenarla per riconoscere la differenza tra ramen e spaghetti e 
le prima 10.000 immagini fossero tutte di ramen avrei un problema perchè l'AI all'inizio 
calibrerebbe il tensore per ricnoscere solo il ramen.
Eseguire uno shuffle quindi ci permetterebbe di allenare l'AI su entrambi i tipi di Ramen in
contemporanea.

Ovviamente se gli do un seed fisso la randomizzazione sarà sempre uguale come nel caso di shuffle_2

L'utilizzo di un seed globale è utile quando si vuole ottenere una riproducibilità dei risultati in diverse parti del codice, ad esempio quando si esegue un'operazione di addestramento di una rete neurale in più riprese. D'altra parte, l'utilizzo di un seed locale può essere utile quando si vuole controllare il comportamento di una singola operazione senza influenzare il resto del codice.
Se non è impostato né il seed globale né il seed dell'operazione: per questa operazione viene utilizzato un seed scelto casualmente.
Se il seed globale è impostato, ma il seed dell'operazione non lo è: il sistema seleziona deterministicamente un seed dell'operazione insieme al seed globale in modo da ottenere una sequenza casuale univoca. All'interno della stessa versione di tensorflow e del codice utente, questa sequenza è deterministica. Tuttavia, nelle diverse versioni, questa sequenza potrebbe cambiare. Se il codice dipende da particolari seed per funzionare, specificare in modo esplicito sia i seed globali che quelli a livello di operazione.
Se il seed dell'operazione è impostato, ma il seed globale non lo è: per determinare la sequenza casuale vengono utilizzati un seed globale predefinito e il seed dell'operazione specificato.
Se sono impostati sia il seed globale che quello dell'operazione: entrambi i semi vengono utilizzati insieme per determinare la sequenza casuale.

-----------------------Creating Array with numpy arrays------------------------

La differenza principale tra i tensor di Tensorflow e gli array di Numpy è che
i tensori hanno una meaggior velocità se esguiti tramite la GPU.

Tutto ciò che è scritto in numpy può essere riscritto con Tf.
Generalmente con numpy si mettonbo le variabili in minuscolo mentre con tensorflow in maiuscolo

Il codice crea tre tensori utilizzando TensorFlow e un array NumPy.

1. `x`: un tensore di dimensione 10x7, contenente solo valori 1.
2. `y`: un tensore di dimensione 4x3, contenente solo valori 0.
3. `numpy_A`: un array NumPy contenente numeri interi che vanno da 1 a 24 (estremi inclusi).
4. `A`: un tensore costante creato a partire dall'array `numpy_A`.
5. `B`: un tensore costante creato a partire dall'array `numpy_A` con una forma specifica (2x3x4).
6. `C`: un tensore costante con gli stessi valori del B m,a a cui è stata rimodellata la forma. (8x3 fa sempre 24, avrei potuto dargli anche  2x2x2x3 ad esempio)

OUTPUT A
tf.Tensor([ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24], shape=(24,), dtype=int32)

OUTPUT B
tf.Tensor(
[[[ 1  2  3  4]
  [ 5  6  7  8]
  [ 9 10 11 12]]

 [[13 14 15 16]
  [17 18 19 20]
  [21 22 23 24]]], shape=(2, 3, 4), dtype=int32)
OUTPUT C
tf.Tensor(
[[ 1  2  3]
 [ 4  5  6]
 [ 7  8  9]
 [10 11 12]
 [13 14 15]
 [16 17 18]
 [19 20 21]
 [22 23 24]], shape=(8, 3), dtype=int32)

 --------------------Tensor attributes-------------------------------

Il codice crea un tensore di rango 4 e ne mostra alcune proprietà. Spiegherò ogni riga del codice:

1. `rank_4_tensor = tf.zeros(shape=[2,3,4,5])`: Crea un tensore di rango 4 con tutti gli elementi impostati a 0. La forma del tensore è [2, 3, 4, 5], che rappresenta 2 matrici, ognuna contenente 3 matrici di dimensione 4x5.
2. `print(rank_4_tensor)`: Stampa il tensore completo, mostrando la sua forma e i valori contenuti in esso.
3. `print(rank_4_tensor[0])`: Stampa solo la prima delle due matrici esterne (indice 0).
4. `print(rank_4_tensor[1][1][3][2])`: Accede a un singolo elemento del tensore usando gli indici [1, 1, 3, 2] e lo stampa. Poiché il tensore è inizializzato con tutti gli elementi impostati a 0, il risultato sarà 0.
5. `print(rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor))`: Stampa la forma, il rango e la dimensione del tensore. La forma è [2, 3, 4, 5], il rango è 4 (poiché ha 4 dimensioni), e la dimensione è il prodotto delle lunghezze delle dimensioni (ovvero 2 * 3 * 4 * 5 = 120), che rappresenta il numero totale di elementi nel tensore.
6. `print(rank_4_tensor.shape[-1])` rappresenta la forma dell'elemento -1
Il numero di dimensio in parole provere indica il numero di numeri da cui è composta la shape. es: shape=(7,8,9,6) abrà ndim = 4.

--------------------indexing and expanding tensors-------------------

Il codice crea un tensore di rango 4, esegue alcune operazioni di slicing e manipolazione delle dimensioni, e ne mostra i risultati. Ecco un riassunto delle operazioni:

1. Crea un tensore di rango 4 `rank_4_tensor` di forma [2, 3, 4, 5], con tutti gli elementi impostati a 0.
2. Crea un nuovo tensore `x` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `x` ha forma [2, 2, 2, 2].
3. Crea un nuovo tensore `y` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `y` ha forma [1, 1, 4, 5].
4. Crea un nuovo tensore `z` estraendo gli elementi da `rank_4_tensor` utilizzando slicing; `z` ha forma [1, 1, 4, 1].
5. Crea un tensore di rango 2 `rank_2_tensor` di forma [2, 2].
6. Crea un nuovo tensore `za` estraendo l'ultima colonna di `rank_2_tensor` utilizzando slicing; `za` ha forma [2].
7. Espande le dimensioni di `rank_2_tensor` utilizzando `tf.newaxis`, creando `rank_2_tensor_expanded` di forma [2, 2, 1].
8. Espande le dimensioni di `rank_2_tensor` utilizzando `tf.expand_dims`, creando `other_rank_2_tensor_expanded` di forma [2, 2, 1].

NB: ESAPANDERE LE DIMENSIONI NON VUOL DIRE AGGIUNGERE MATRICI, MA VUOL DIRE AGGIUNGERE 
DIMENSIONI AGLI ALEMENTI ESISTENTI, QUIDI ANDARLI A SCOMPATTARE IN SOTTOMATRICI.

------------------manipulating tensors with basic algebra--------------

1. La riga `tensor = tf.constant([[10,7],[3,4]])` crea un tensore costante con valori [[10,7],[3,4]] e lo assegna alla variabile `tensor`.
2. La riga `print(tensor + 10)` esegue un'operazione di addizione tra il tensore `tensor` e il valore scalare 10. Il risultato è un nuovo tensore in cui a ciascun elemento del tensore originale viene sommato 10. Viene stampato il risultato.
3. La riga `print(tensor * 100)` esegue un'operazione di moltiplicazione tra il tensore `tensor` e il valore scalare 100. Il risultato è un nuovo tensore in cui ciascun elemento del tensore originale viene moltiplicato per 100. Viene stampato il risultato.
4. La riga `print(tensor - 10)` esegue un'operazione di sottrazione tra il tensore `tensor` e il valore scalare 10. Il risultato è un nuovo tensore in cui a ciascun elemento del tensore originale viene sottratto 10. Viene stampato il risultato.
5. La riga `print(tf.multiply(tensor,10))` esegue un'operazione di moltiplicazione tra il tensore `tensor` e il valore scalare 10 utilizzando la funzione `tf.multiply()`. Il risultato è un nuovo tensore in cui ciascun elemento del tensore originale viene moltiplicato per 10. Viene stampato il risultato.

------------------matrix manipulation with tensor -------------------------

1. La riga `tensor = tf.constant([[10,7],[3,4]])` crea un tensore costante di dimensione 2x2 con i valori specificati e lo assegna alla variabile `tensor`.
2. La riga `x = tf.matmul(tensor, tensor)` esegue una moltiplicazione di matrici tra il tensore `tensor` e se stesso utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `x`.
3. La riga `print(x)` stampa il valore del tensore `x`, che è il risultato della moltiplicazione di matrici.
4. Le righe seguenti definiscono due nuovi tensori costanti: `tensor_1` e `tensor_2`.
5. La riga `y = tf.matmul(tensor_1, tensor_2)` esegue una moltiplicazione di matrici tra `tensor_1` e `tensor_2` utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `y`.
6. La riga `print(y)` stampa il valore del tensore `y`, che è il risultato della moltiplicazione di matrici.

È importante notare che la libreria TensorFlow supporta la notazione `@` per la moltiplicazione di matrici, quindi l'istruzione `y = tf.matmul(tensor_1, tensor_2)` può essere scritta anche come `y = tensor_1 @ tensor_2`.
Infine, come ho menzionato, per la moltiplicazione di matrici e la notazione puntata (dot notation), valgono le stesse regole di NumPy(che sono le stesse regole del prodotto tra matrici in generale). 

7. La riga `tensor_3 = tf.reshape(tensor_2, shape=(2,3))` ridimensiona il tensore `tensor_2` in una nuova forma 2x3 e lo assegna alla variabile `tensor_3`.
8. La riga `trasposta_tensor_3 = tf.transpose(tensor_3)` calcola la trasposta del tensore `tensor_3` e la assegna alla variabile `trasposta_tensor_3`.
9. La riga `print(tensor_3, trasposta_tensor_3)` stampa il valore dei tensori `tensor_3` e `trasposta_tensor_3`.
10. La riga `z = tf.matmul(tensor_2, tensor_3)` esegue una moltiplicazione di matrici tra `tensor_2` e `tensor_3` utilizzando la funzione `tf.matmul()`. Il risultato è un nuovo tensore che rappresenta il prodotto di matrici. Viene assegnato alla variabile `z`.
11. La riga `print(z)` stampa il valore del tensore `z`, che è il risultato della moltiplicazione di matrici.
12. La riga `za= tf.tensordot(tensor_2, tensor_3, axes= 2)` esegue un'operazione di tensordot tra `tensor_2` e `tensor_1` con una contrazione lungo 2 assi. Il risultato è un singolo scalare che viene assegnato alla variabile `za`.
13. La riga `print(za)` stampa il valore del tensore `za`, che è il risultato dell'operazione tensordot.

------------------------changing the datatype of tensors----------------------------

In Tensorflow, il termine "dtype" si riferisce al "data type" (tipo di dato) utilizzato per rappresentare i numeri all'interno dei tensori. I dtype più comuni in Tensorflow sono "float" (per numeri in virgola mobile) e "int" (per numeri interi), e possono avere diverse lunghezze, come 16, 32 o 64 bit. La scelta del dtype ha un impatto sulle prestazioni e sulla precisione dei calcoli.

1. 16 bit: 
- float16 (o "half precision"): questo tipo di dato utilizza 16 bit per rappresentare un numero in virgola mobile. Ha una gamma e una precisione limitate rispetto ai dtype a 32 o 64 bit. Tuttavia, poiché utilizza meno memoria e richiede meno larghezza di banda, può offrire un aumento delle prestazioni in alcuni modelli, specialmente su hardware specifico che supporta operazioni a 16 bit, come le GPU NVIDIA con supporto Tensor Core.
- int16: simile al float16, questo tipo di dato utilizza 16 bit per rappresentare numeri interi. Ha una gamma limitata rispetto agli interi a 32 o 64 bit, ma occupa meno memoria.

2. 32 bit:
- float32 (o "single precision"): questo tipo di dato utilizza 32 bit per rappresentare un numero in virgola mobile. È il dtype più comune utilizzato in Tensorflow e offre un buon equilibrio tra precisione e consumo di memoria.
- int32: questo tipo di dato utilizza 32 bit per rappresentare numeri interi. È molto più comune di int16 e offre una gamma di valori molto più ampia.

3. 64 bit:
- float64 (o "double precision"): questo tipo di dato utilizza 64 bit per rappresentare un numero in virgola mobile. Offre una precisione molto elevata, ma a scapito di un maggiore consumo di memoria e potenzialmente prestazioni inferiori rispetto a float32.
- int64: questo tipo di dato utilizza 64 bit per rappresentare numeri interi e offre una gamma di valori enorme. In genere, non è necessario utilizzare int64 per la maggior parte delle applicazioni di deep learning, a meno che non sia richiesta una gamma di valori estremamente ampia.

In sintesi, la scelta del dtype dipende dalle esigenze di precisione e dalle risorse hardware disponibili. Nella maggior parte dei casi, si consiglia di utilizzare float32 per i numeri in virgola mobile e int32 per i numeri interi. Tuttavia, in alcune situazioni, potrebbe essere utile utilizzare dtype a 16 o 64 bit per migliorare le prestazioni o la precisione.


La "gamma di valori" si riferisce all'intervallo di numeri che un tipo di dato può rappresentare. Ogni tipo di dato ha una capacità diversa di rappresentare numeri in base al numero di bit utilizzati per codificarli. Maggiore è il numero di bit, maggiore è la gamma di valori che il tipo di dato può rappresentare.
Per esempio, consideriamo i tipi di dati interi (int):

1. int16: con 16 bit, si possono rappresentare 2^16 valori distinti, che vanno da -32.768 a 32.767 (inclusi sia il numero positivo che quello negativo, poiché il primo bit è utilizzato per il segno).
2. int32: con 32 bit, si possono rappresentare 2^32 valori distinti, che vanno da -2.147.483.648 a 2.147.483.647.
3. int64: con 64 bit, si possono rappresentare 2^64 valori distinti, che vanno da -9.223.372.036.854.775.808 a 9.223.372.036.854.775.807.

Per i numeri in virgola mobile (float), la gamma di valori è determinata dalla distribuzione dei bit tra l'esponente e la mantissa (cioè la parte intera e quella frazionaria del numero). Ad esempio:
1. float16: conosciuto anche come "half precision", utilizza 1 bit per il segno, 5 bit per l'esponente e 10 bit per la mantissa. Ha una gamma di valori più limitata e una precisione inferiore rispetto a float32 e float64.
2. float32: conosciuto anche come "single precision", utilizza 1 bit per il segno, 8 bit per l'esponente e 23 bit per la mantissa. Ha una gamma di valori e una precisione intermedia tra float16 e float64.
3. float64: conosciuto anche come "double precision", utilizza 1 bit per il segno, 11 bit per l'esponente e 52 bit per la mantissa. Ha la gamma di valori e la precisione più ampie tra questi tre tipi di dati in virgola mobile.
In generale, la gamma di valori di un tipo di dato è importante perché determina quale intervallo di numeri può essere rappresentato e gestito in modo accurato dall'algoritmo. Se si lavora con numeri al di fuori della gamma di valori di un tipo di dato, si possono verificare errori di approssimazione, overflow o underflow, che possono influire sulla precisione e sulle prestazioni dell'algoritmo.

In questo frammento di codice, vengono create due costanti Tensorflow (B e C) e successivamente vengono convertite (cast) in tipi di dati diversi (B_cast e C_cast). 
1. B è un tensore di tipo float32 con due numeri in virgola mobile (1.7 e 7.4).
2. C è un tensore di tipo int32 con due numeri interi (7 e 10).
Successivamente, si utilizza la funzione `tf.cast()` per convertire i tensori B e C in tipi di dati diversi:
3. B_cast è il risultato della conversione di B in un tensore di tipo float16.
4. C_cast è il risultato della conversione di C in un tensore di tipo float64.
Infine, il codice stampa i dtype di B, C, B_cast e C_cast, mostrando che B è di tipo float32, C è di tipo int32, B_cast è di tipo float16 e C_cast è di tipo float64.

------------------------------------tensor aggregations--------------------------------

Il codice fa quanto segue:

1. Crea un tensore `D` con due valori interi (-7 e -10).
2. Calcola il valore assoluto di ogni elemento nel tensore `D` utilizzando `tf.abs()` e assegna il risultato a `D_abs`.
3. Stampa il tensore `D_abs` che contiene i valori assoluti degli elementi in `D`.
4. Crea un tensore di numeri casuali (`tensor_random`) con una forma di (2, 3, 3) e valori compresi tra 0 e 10 (incluso 0, escluso 10), utilizzando un seed di 42 per la generazione di numeri casuali.
5. Stampa il tensore `tensor_random`.
6. Calcola e stampa il valore minimo nel tensore `tensor_random` usando `tf.reduce_min()`.
7. Calcola e stampa il valore massimo nel tensore `tensor_random` usando `tf.reduce_max()`.
8. Calcola e stampa il valore medio nel tensore `tensor_random` usando `tf.reduce_mean()`.
9. Calcola e stampa la somma degli elementi nel tensore `tensor_random` usando `tf.reduce_sum()`.
10. Calcola la varianza degli elementi nel tensore `tensor_random` usando `tf.math.reduce_variance()` e assegna il risultato a `tensor_random_variance`.
11. Calcola la deviazione standard degli elementi nel tensore `tensor_random` utilizzando `tf.math.reduce_std`, (non nel mio caso ma potrebbe servire castare il tensore su qui stiamo calcolando la sd). In ogni caso avrei potuto anche fare la stessa operazione andando a usare sqrt sulla varianza.
12. Calcola la varianza nel sensore tensor_random usando  `tfp.stats.variance` che calcola la varianza lungo l'ultimo asse del tensore per default. In altre parole, calcola la varianza per ogni "gruppo" di valori lungo l'ultimo asse, restituendo un array di varianze invece di un singolo valore.
(volendo avrei potuto anche calcolare la sd con l'ultimo metodo semplicemente facendo un reshape del tensore)

--------Finding the positional minimum and maximum of a tensor (argmin e argmax)---------------

1. Crea un tensore `F` con 50 valori casuali generati uniformemente. La forma del tensore è [50].
2. Trova l'indice del valore massimo nel tensore `F` utilizzando la funzione `tf.argmax()` e assegna il risultato a `F_max`.
3. Trova l'indice del valore minimo nel tensore `F` utilizzando la funzione `tf.argmin()` e assegna il risultato a `F_min`.
4. Stampa gli indici del valore massimo e minimo nel tensore `F` (rispettivamente `F_max` e `F_min`).

------------------------------Sqeezing a tensor (removing all single dimensions)---------------

Il codice fa quanto segue:

1. Crea un tensore `G` con 50 valori casuali generati uniformemente. Inoltre, il tensore viene ridimensionato con una forma di (1, 1, 1, 1, 50) aggiungendo dimensioni singoleton (di lunghezza 1) lungo i primi quattro assi.
2. Stampa il tensore `G`.

3. Utilizza la funzione `tf.squeeze()` per rimuovere tutte le dimensioni singoleton (di lunghezza 1) dal tensore `G` e assegna il risultato a `G_squeezed`.
4. Stampa il tensore `G_squeezed`.

In sintesi, il codice crea un tensore con valori casuali e una forma specifica (1, 1, 1, 1, 50), poi rimuove tutte le dimensioni inutili (le dimensioni singoleton) utilizzando `tf.squeeze()` e stampa il tensore risultante.

-------------------------------One-hot encoding--------------------------------------------

Il codice fa quanto segue:

1. Crea una lista di etichette categoriche (in questo caso, classi di numeri interi) chiamata `list`, che contiene 4 elementi: 0, 1, 2 e 3.
2. Definisce il numero di classi uniche come `num_classes` (in questo caso, ci sono 4 classi: 0, 1, 2 e 3).
3. Esegue l'one-hot encoding delle etichette presenti nella lista `list` utilizzando la funzione `tf.one_hot()`. Il parametro `depth` è impostato sul numero di classi (`num_classes`), che in questo caso è 4. Il risultato viene assegnato a `one_hot_encoded`.
4. Stampa il tensore `one_hot_encoded`, che contiene i vettori one-hot corrispondenti a ciascuna etichetta nella lista `list`.

In sintesi, il codice converte una lista di etichette categoriche (0, 1, 2, 3) in vettori one-hot encoded utilizzando la funzione `tf.one_hot()` e stampa il risultato.
(in ogni caso oltre a 0 1 gli posso dare io un on value e un off value)

--------------------------------More math operations--------------------------------------

Il codice esegue le seguenti operazioni:

1. Crea un tensore `H` contenente una sequenza di valori interi da 1 a 9 utilizzando `tf.range(1, 10)`.
2. Calcola il quadrato di ogni elemento nel tensore `H` utilizzando `tf.square(H)` e lo assegna a `H_square`.
3. Converte il tipo di dati degli elementi nel tensore `H_square` in interi a 16 bit (`tf.int16`) utilizzando `tf.cast(H_square, dtype=tf.int16)` e lo assegna a `H_square_cast`.
4. Converte il tipo di dati degli elementi nel tensore `H` in numeri a virgola mobile a 32 bit (`tf.float32`) utilizzando `tf.cast(H, dtype=tf.float32)`.
5. Calcola il logaritmo naturale di ogni elemento nel tensore convertito e lo assegna a `H_log` utilizzando `tf.math.log()`.

------------------------------TF with NP------------------------------------------------

Il codice esegue le seguenti operazioni:

1. Crea un tensore `J` utilizzando un array NumPy di numeri in virgola mobile a 64 bit (dtype `float64`) con i valori `[3., 7., 10.]`.
2. Converte il tensore `J` in un array NumPy `J_n` utilizzando `np.array(J)`.
3. Stampa i tipi di dati (dtype) di `J` e `J_n` per mostrare che entrambi hanno lo stesso dtype (`float64`).
4. Crea un tensore `K` utilizzando una lista di numeri in virgola mobile con i valori `[3., 7., 10.]`. TensorFlow userà il dtype predefinito per numeri a virgola mobile, che è `float32`.
5. Stampa il tipo di dato (dtype) di `K`, mostrando che ha un dtype `float32`.

In sintesi, questo esempio dimostra che il dtype predefinito per numeri a virgola mobile in NumPy è `float64`, mentre il dtype predefinito in TensorFlow è `float32`.

--------------------------------------TF vs NUMPY--------------------------------------

Nel codice vediamo la differenza di esecuzione tra numpy e tf, usando pip install tensorflow-directml-plugin per usare la
GPU abbiamo 0.3 second per FT contro 15.5 per numpy. Disinstallandolo abbiamo 15 per numpy e 9 per TF.
Tf batte Np anche su CPU perchè è ottimizzato per il machine learning quindi per sto tipo di operazioni.

Con tf.config.list_physical_devices() possiamo vedere se TFsta girando su CPU o su GPU. Nel mio caso avendo abilitato la GPU rileva
sia la GPU che la CPU, se la disabilitassi con pip uninstall tensorflow-directml-plugin rileverebbe solo la CPU.
Per impostazione predefinita, TensorFlow sceglierà la GPU se disponibile, altrimenti utilizzerà la CPU. 
Quindi, nel mio caso, TensorFlow utilizzerà la GPU per l'esecuzione, a meno che non venga specificato diversamente.

Le GPU AMD Radeon, come la Radeon RX 5700, non utilizzano l'architettura CUDA di NVIDIA. CUDA (Compute Unified Device Architecture) è una piattaforma di calcolo parallelo e un modello di programmazione specifico per le GPU NVIDIA.
Per le GPU AMD, viene utilizzata una piattaforma di calcolo aperta chiamata ROCm (Radeon Open Compute). ROCm offre un ambiente di calcolo parallelo e un modello di programmazione per GPU AMD simile a CUDA per GPU NVIDIA. ROCm supporta anche il linguaggio di programmazione OpenCL, che è un linguaggio di programmazione standard e aperto per il calcolo parallelo su diverse piattaforme, tra cui CPU, GPU e altri acceleratori.
Se vuoi eseguire workload basati su TensorFlow su GPU AMD, dovrai utilizzare TensorFlow-ROCm, una versione di TensorFlow ottimizzata per GPU AMD. Puoi trovare ulteriori informazioni su TensorFlow-ROCm e su come installarlo nella documentazione ufficiale di ROCm: https://rocmdocs.amd.com/en/latest/Deep_learning/Deep-learning.html#tensorflow
